### Q2: An extension of the previous Question -  
  
Perform a single sqoop import inside the directory in hdfs named sqoop_importdir, considering all the following points:  
  
● Import all the columns except Cust_Type in hdfs.  
● Include only the purchases made after 2019-01-01  
● The output data generated should have fields separated by | and rows separated by ; (semicolon)  
● While importing, Nulls in the data , should be overridden with ‘NA’  
● Redirect the log messages generated on screen to the files log_out1 and log_out2. Display the contents of the log_out2 file , when sqoop import is
successful,share the snapshot of the number of records retrieved.  
● Display the contents of the sqoop_importdir  
● Now Again modify and run your sqoop import command ,so that cust_id column can be used to decide the input splits, as the Primary key column is not proper.
Also ensure that the output directory remains as sqoop_importdir, and the previously imported contents are automatically deleted and new contents are
filled in the output directory.  
● Display the contents of the output directory now and the first 10 records from the mapper output files (hint: use head command)  
● Now Suppose an outlier comes into the mysql table:  
The new record inserted is :  
  Cust_Id Customer_Name Purchase_Date Item City Price Cust_Type  
  10000 Raman 2019/09/04 Misc Cochin 9000 Regular  
    
Mention the sqoop import command you will frame from your end to deal with such a situation to ensure even work distribution among mappers, using customized
bounding val query.  
#### Note: you got to know that cust_id 10000 is erroneous record and should not be taken care.
